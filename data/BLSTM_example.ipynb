{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zisrGCcZUkdU"
   },
   "outputs": [],
   "source": [
    "# Attention-BLSTM classification example\n",
    "\n",
    "# import the required modules\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "tf.config.experimental.set_visible_devices(devices=gpus[0], device_type='GPU')\n",
    "tf.config.experimental.set_memory_growth(device=gpus[0], enable=True)\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adamax\n",
    "from keras import initializers\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize,LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Attention implemented by https://github.com/CyberZHG/keras-self-attention\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "# turn off the warnings, be careful when use this\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "# parameters to be investigated in grid seearch\n",
    "time_steps = [1,5,10]  # input history to include: [1,5,10]\n",
    "lstm_sizes = [[16,8,4],[32,16,8],[64,32,16]] # number of neurons in the BLSTM layers: [[16,8,4],[32,16,8],[64,32,16]]\n",
    "attention_widths = [1,2,4] # width of the local context for the attention layer: [1,2,4]\n",
    "\n",
    "# other parameters\n",
    "batch_size = 32 # for estimating error gradient\n",
    "# number of total epochs to train the model\n",
    "nb_epoch = 20\n",
    "# optimization function\n",
    "opt_func = Adamax(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "# to prevent over-fitting\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape panda.DataFrame to Keras style: (batch_size, time_step, nb_features)\n",
    "def reshape_data(data, n_prev):\n",
    "    docX = []\n",
    "    for i in range(len(data)):\n",
    "        if i < (len(data)-n_prev):\n",
    "            docX.append(data[i:i+n_prev])\n",
    "        else: # the frames in the last window use the same context\n",
    "            docX.append(data[(len(data)-n_prev):len(data)])\n",
    "    alsX = np.array(docX)\n",
    "    return alsX\n",
    "\n",
    "# define the BLSTM model with attention\n",
    "def attBLSTM(lstm_size, attention_width, nb_class, opt_func):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=lstm_size[0], return_sequences=True))) # BLSTM layer 1\n",
    "    #model.add(RNN(LSTMCell(lstm_size[0]), return_sequences=True), kernel_initializer=initializers.glorot_uniform(seed=0))\n",
    "    model.add(Bidirectional(LSTM(units=lstm_size[1], return_sequences=True))) # BLSTM layer 2\n",
    "    #model.add(RNN(LSTMCell(lstm_size[1]), return_sequences=True), kernel_initializer=initializers.glorot_uniform(seed=0))\n",
    "    model.add(Bidirectional(LSTM(units=lstm_size[2], return_sequences=True))) # BLSTM layer 3\n",
    "    #model.add(RNN(LSTMCell(lstm_size[2]), return_sequences=True), kernel_initializer=initializers.glorot_uniform(seed=0))\n",
    "    model.add(SeqSelfAttention(attention_width=attention_width, attention_activation='sigmoid')) # attention layer\n",
    "    model.add(Dense(units=nb_class, activation='softmax')) # output layer, predict emotion dimensions seperately\n",
    "    return model\n",
    "\n",
    "# one-hot encoding of the class labels\n",
    "def one_hot(labels, cat):\n",
    "    labels_converted = []\n",
    "    if cat == 'BASE':\n",
    "        for label in labels:\n",
    "            if label == 'STATIONARY':\n",
    "                label_converted = [1,0,0,0]\n",
    "            elif label == 'TO OPERATOR':\n",
    "                label_converted = [0,1,0,0]\n",
    "            elif label == 'ROTATING':\n",
    "                label_converted = [0,0,1,0]\n",
    "            elif label == 'TO PARTICIPANT':\n",
    "                label_converted = [0,0,0,1]\n",
    "            labels_converted.append(label_converted)\n",
    "    elif cat == 'ARM':\n",
    "        for label in labels:\n",
    "            if label == 'STATIONARY':\n",
    "                label_converted = [1,0,0]\n",
    "            elif label == 'REACHING':\n",
    "                label_converted = [0,1,0]\n",
    "            elif label == 'TUCKING':\n",
    "                label_converted = [0,0,1]\n",
    "            labels_converted.append(label_converted)\n",
    "    elif cat == 'OTP':\n",
    "        for label in labels:\n",
    "            if label == 'LEFT':\n",
    "                label_converted = [1,0,0]\n",
    "            elif label == 'MIDDLE':\n",
    "                label_converted = [0,1,0]\n",
    "            elif label == 'RIGHT':\n",
    "                label_converted = [0,0,1]\n",
    "            labels_converted.append(label_converted)\n",
    "    labels_converted = np.asarray(labels_converted)\n",
    "    return labels_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the best performing model\n",
    "def model_eval(f_mode, c_mode, log_f, model, time_step):\n",
    "    # X_trn = reshape_data(globals()['x_'+str(f_mode)+'_trn'], time_step)\n",
    "    X_dev = reshape_data(globals()['x_'+str(f_mode)+'_dev'], time_step)\n",
    "    X_tst = reshape_data(globals()['x_'+str(f_mode)+'_tst'], time_step)\n",
    "    # Y_trn = reshape_data(globals()['y_'+str(c_mode)+'_trn'], time_step)\n",
    "    Y_dev = reshape_data(globals()['y_'+str(c_mode)+'_dev'], time_step)\n",
    "    Y_tst = reshape_data(globals()['y_'+str(c_mode)+'_tst'], time_step)\n",
    "\n",
    "    # print results on training set\n",
    "    # model.evaluate(X_trn, Y_trn, batch_size=batch_size)\n",
    "    # trn_pred = model.predict(X_trn)\n",
    "    # y_trn_non_category = [ np.argmax(t[0]) for t in Y_trn ]\n",
    "    # y_trn_predict_non_category = [ np.argmax(t[0]) for t in trn_pred ]\n",
    "    # print('Confusion Matrix on train set')\n",
    "    # print(confusion_matrix(y_trn_non_category, y_trn_predict_non_category))\n",
    "    # trn_f1 = f1_score(y_trn_non_category, y_trn_predict_non_category, average='weighted')\n",
    "    # print('Weighted F1-score on train set:', trn_f1)\n",
    "    # with open(log_f, 'a') as logfile:\n",
    "        # logfile.write('Confusion Matrix on train set\\n')\n",
    "        # np.savetxt(logfile, confusion_matrix(y_trn_non_category, y_trn_predict_non_category))\n",
    "        # logfile.write('Weighted F1-score on train set: %s' % trn_f1)\n",
    "\n",
    "    # print results on dev set\n",
    "    model.evaluate(X_dev, Y_dev, batch_size=batch_size)\n",
    "    dev_pred = model.predict(X_dev)\n",
    "    y_dev_non_category = [ np.argmax(t[0]) for t in Y_dev ]\n",
    "    y_dev_predict_non_category = [ np.argmax(t[0]) for t in dev_pred ]\n",
    "    print('Confusion Matrix on development set')\n",
    "    print(confusion_matrix(y_dev_non_category, y_dev_predict_non_category))\n",
    "    dev_f1 = f1_score(y_dev_non_category, y_dev_predict_non_category, average='weighted')\n",
    "    print('Weighted F1-score on development set:', dev_f1)\n",
    "    with open(log_f, 'a') as logfile:\n",
    "        logfile.write('Confusion Matrix on development set\\n')\n",
    "        np.savetxt(logfile, confusion_matrix(y_dev_non_category, y_dev_predict_non_category))\n",
    "        logfile.write('Weighted F1-score on development set: %s' % dev_f1)\n",
    "\n",
    "    # print results on test set\n",
    "    model.evaluate(X_tst, Y_tst, batch_size=batch_size)\n",
    "    tst_pred = model.predict(X_tst)\n",
    "    y_tst_non_category = [ np.argmax(t[0]) for t in Y_tst ]\n",
    "    y_tst_predict_non_category = [ np.argmax(t[0]) for t in tst_pred ]\n",
    "    print('Confusion Matrix on test set')\n",
    "    print(confusion_matrix(y_tst_non_category, y_tst_predict_non_category))\n",
    "    tst_f1 = f1_score(y_tst_non_category, y_tst_predict_non_category, average='weighted')\n",
    "    print('Weighted F1-score on test set:', tst_f1)\n",
    "    with open(log_f, 'a') as logfile:\n",
    "        logfile.write('Confusion Matrix on test set\\n')\n",
    "        np.savetxt(logfile, confusion_matrix(y_tst_non_category, y_tst_predict_non_category))\n",
    "        logfile.write('Weighted F1-score on test set: %s' % tst_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data files\n",
    "file_trn = 'data/ML/combined_ML_trn.csv'\n",
    "file_dev = 'data/ML/combined_ML_dev.csv'\n",
    "file_tst = 'data/ML/combined_ML_tst.csv'\n",
    "\n",
    "# read in data\n",
    "trn_data = pd.read_csv(file_trn, header=0)\n",
    "dev_data = pd.read_csv(file_dev, header=0)\n",
    "tst_data = pd.read_csv(file_tst, header=0)\n",
    "\n",
    "# number of features\n",
    "# nb_feat_time = 3 # ['time (s)', 'episode', 'step']\n",
    "# nb_feat_kp_cor = 75 # (x,y,z) of the 25 facial and upper body keypoints\n",
    "# nb_feat_kp_con = 25 # confidence of the facial and upper body keypoints\n",
    "# nb_feat_task = 1 # task progress\n",
    "# nb_feat_emo = 20 # categorical and arousal-valence for both cameras\n",
    "# nb_feat_rw = 3 # reward values: operator's rating, emotional reward, combined\n",
    "# nb_feat_all = nb_feat_time + nb_feat_kp_cor + nb_feat_kp_con + nb_feat_task + nb_feat_emo + nb_feat_rw\n",
    "# number of classes\n",
    "nb_class_base = 4 # {'STATIONARY', 'TO OPERATOR', 'ROTATING', 'TO PARTICIPANT'}\n",
    "nb_class_arm = 3 # {'STATIONARY', 'REACHING', 'TUCKING'}\n",
    "nb_class_otp = 3 # {'MIDDLE', 'LEFT', 'RIGHT'}\n",
    "\n",
    "x_all_trn = trn_data.iloc[:,:127]\n",
    "# drop S1 handover episodes in training set: episodes [0,1,2,3]\n",
    "trn_no_S1_data = trn_data[trn_data.episode > 3]\n",
    "X_no_S1_trn = trn_no_S1_data.iloc[:,:127]\n",
    "# for ablation studies\n",
    "x_no_time_trn = trn_data.iloc[:,3:127]\n",
    "x_no_rw_trn = trn_data.iloc[:,3:124]\n",
    "x_no_emo_trn = trn_data.iloc[:,23:124]\n",
    "x_no_tp_trn = trn_data.iloc[:,23:123]\n",
    "trn_data = trn_data[trn_data.columns.drop(list(trn_data.filter(regex='(confidence)')))]\n",
    "x_no_conf_trn = trn_data.iloc[:,23:98]\n",
    "\n",
    "x_all_dev = dev_data.iloc[:,:127]\n",
    "# for ablation studies\n",
    "x_no_time_dev = dev_data.iloc[:,3:127]\n",
    "x_no_rw_dev = dev_data.iloc[:,3:124]\n",
    "x_no_emo_dev = dev_data.iloc[:,23:124]\n",
    "x_no_tp_dev = dev_data.iloc[:,23:123]\n",
    "dev_data = dev_data[dev_data.columns.drop(list(dev_data.filter(regex='(confidence)')))]\n",
    "x_no_conf_dev = dev_data.iloc[:,23:98]\n",
    "\n",
    "x_all_tst = tst_data.iloc[:,:127]\n",
    "# for ablation studies\n",
    "x_no_time_tst = tst_data.iloc[:,3:127]\n",
    "x_no_rw_tst = tst_data.iloc[:,3:124]\n",
    "x_no_emo_tst = tst_data.iloc[:,23:124]\n",
    "x_no_tp_tst = tst_data.iloc[:,23:123]\n",
    "tst_data = tst_data[tst_data.columns.drop(list(tst_data.filter(regex='(confidence)')))]\n",
    "x_no_conf_tst = tst_data.iloc[:,23:98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of the classes\n",
    "y_base_trn = one_hot(trn_data['base status'], cat = 'BASE')\n",
    "y_base_dev = one_hot(dev_data['base status'], cat = 'BASE')\n",
    "y_base_tst = one_hot(tst_data['base status'], cat = 'BASE')\n",
    "y_base_no_S1_trn = one_hot(trn_no_S1_data['base status'], cat = 'BASE')\n",
    "\n",
    "y_arm_trn = one_hot(trn_data['arm status'], cat = 'ARM')\n",
    "y_arm_dev = one_hot(dev_data['arm status'], cat = 'ARM')\n",
    "y_arm_tst = one_hot(tst_data['arm status'], cat = 'ARM')\n",
    "y_arm_no_S1_trn = one_hot(trn_no_S1_data['arm status'], cat = 'ARM')\n",
    "\n",
    "y_otp_trn = one_hot(trn_data['handover status'], cat = 'OTP')\n",
    "y_otp_dev = one_hot(dev_data['handover status'], cat = 'OTP')\n",
    "y_otp_tst = one_hot(tst_data['handover status'], cat = 'OTP')\n",
    "y_otp_no_S1_trn = one_hot(trn_no_S1_data['handover status'], cat = 'OTP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode control for feature set and class\n",
    "time_stamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "c_mode_list = ['base', 'arm', 'otp']\n",
    "\n",
    "f_mode = 'all'\n",
    "c_mode = c_mode_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output files\n",
    "file_out = 'exp1/logs/LSTM_' + f_mode + '_' + c_mode + '_sum_' + time_stamp + '.txt'\n",
    "file_log = 'exp1/logs/LSTM_' + f_mode + '_' + c_mode + '_log_' + time_stamp + '.txt'\n",
    "file_pred = 'exp1/logs/LSTM_' + f_mode + '_' + c_mode + '_pred_' + time_stamp + '.txt'\n",
    "model_dir = 'exp1/models/LSTM/' + c_mode + '/' + f_mode + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_list = []\n",
    "tst_pred_list = []\n",
    "f1_list = []\n",
    "count = 1\n",
    "\n",
    "training = False\n",
    "\n",
    "# Grid search on dev set\n",
    "if training:\n",
    "    for time_step in time_steps:\n",
    "        # pad data\n",
    "        X_trn = reshape_data(globals()['x_'+str(f_mode)+'_trn'], time_step)\n",
    "        X_dev = reshape_data(globals()['x_'+str(f_mode)+'_dev'], time_step)\n",
    "        Y_trn = reshape_data(globals()['y_'+str(c_mode)+'_trn'], time_step)\n",
    "        Y_dev = reshape_data(globals()['y_'+str(c_mode)+'_dev'], time_step)\n",
    "        for lstm_size in lstm_sizes:\n",
    "            for attention_width in attention_widths:\n",
    "                para_list.append([time_step, lstm_size, attention_width]) # save parameter set\n",
    "                print('\\n================================ No. %s of 27 ========================================' % count)\n",
    "                print('\\nParameters: time_step = %s, [h1, h2, h3] = %s, attention_width = %s\\n' \n",
    "                      % (time_step, lstm_size, attention_width))\n",
    "                # build model with given parameters\n",
    "                model = attBLSTM(lstm_size, attention_width, globals()['nb_class_'+str(c_mode)], opt_func)\n",
    "                # compile the model\n",
    "                model.compile(loss='categorical_crossentropy', optimizer=opt_func, metrics=['categorical_accuracy'])\n",
    "                # training the model\n",
    "                model.fit(X_trn, Y_trn, batch_size=batch_size, epochs=nb_epoch, \n",
    "                          validation_split=0.05, callbacks=[early_stopping], verbose=2)\n",
    "                # evaluation\n",
    "                model.evaluate(X_dev, Y_dev, batch_size=batch_size)\n",
    "                # save model\n",
    "                model_f = model_dir + str(count)\n",
    "                model.save(model_f)\n",
    "\n",
    "                # save predictions\n",
    "                tst_pred = model.predict(X_dev)\n",
    "                tst_pred_list.append(tst_pred) # save predictions\n",
    "\n",
    "                # print confusion matrix\n",
    "                y_test_non_category = [ np.argmax(t[0]) for t in Y_dev ]\n",
    "                y_predict_non_category = [ np.argmax(t[0]) for t in tst_pred ]\n",
    "                print('Confusion Matrix on dev set')\n",
    "                print(confusion_matrix(y_test_non_category, y_predict_non_category))\n",
    "                tst_f1 = f1_score(y_test_non_category, y_predict_non_category, average='weighted')\n",
    "                f1_list.append(tst_f1) # save f1 score\n",
    "                print('Weighted F1-score on dev set:', tst_f1)\n",
    "                # print grid search log\n",
    "                with open(file_log, 'a') as logfile:\n",
    "                    logfile.write('\\n================================ No. %s of 27 ========================================\\n' % count)\n",
    "                    logfile.write('F1 = %s; Parameters: time_step = %s, [h1, h2, h3] = %s, attention_width = %s\\n' \n",
    "                                  % (tst_f1, time_step, lstm_size, attention_width))\n",
    "                    logfile.write('Confusion Matrix on dev set\\n')\n",
    "                    np.savetxt(logfile, confusion_matrix(y_test_non_category, y_predict_non_category))          \n",
    "                count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best parameter set\n",
    "best = f1_list.index(max(f1_list)) # find the highest F1 score\n",
    "best_count = best + 1\n",
    "result = f1_list[best]\n",
    "para = para_list[best]\n",
    "# prediction = tst_pred_list[best]\n",
    "print('Best Run at No.%s; F1 = %s; Parameters: time_step = %s, [h1,h2,h3] = %s, attention_width = %s\\n' \n",
    "      % (best_count, result, para[0], para[1], para[2]))\n",
    "\n",
    "with open(file_out, 'a') as outfile:\n",
    "    outfile.write('Best Run at No.%s; F1 = %s; Parameters: time_step = %s, [h1,h2,h3] = %s, attention_width = %s\\n' \n",
    "                   % (best_count, result, para[0], para[1], para[2]))\n",
    "\n",
    "save predictions in case of significance test\n",
    "with open(file_pred, 'a') as predfile:\n",
    "    for pred in prediction:\n",
    "        indi_pred = []\n",
    "        indi_pred = pred[0] # reform the seq prediction to individual samples\n",
    "        row = ', '.join(map(str, indi_pred))\n",
    "        predfile.write('%s\\n' % row)\n",
    "\n",
    "# apply on train, dev, and test sets\n",
    "best_model_f = model_dir + str(best_count)\n",
    "best_model = keras.models.load_model(best_model_f)\n",
    "model_eval(f_mode, c_mode, file_out, best_model, para[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ablation study with the best model parameters\n",
    "f_ab_list = ['all', 'no_time', 'no_rw', 'no_emo', 'no_tp', 'no_conf']\n",
    "nb_epoch_ab = 20\n",
    "\n",
    "for f_ab in f_ab_list:\n",
    "    # output files\n",
    "    ab_file_out = 'exp2/logs/LSTM_' + f_ab + '_' + c_mode + '_sum_' + time_stamp + '.txt'\n",
    "    ab_model_dir = 'exp2/models/LSTM/' + c_mode + '/' + f_ab + '/'\n",
    "    with open(ab_file_out, 'a') as outfile:\n",
    "        outfile.write('Parameters: features = %s, class = %s, time_step = %s, [h1,h2,h3] = %s, attention_width = %s\\n' \n",
    "                   % (f_ab, c_mode, para[0], para[1], para[2]))\n",
    "    print('Parameters: features = %s, class = %s, time_step = %s, [h1,h2,h3] = %s, attention_width = %s\\n' \n",
    "          % (f_ab, c_mode, para[0], para[1], para[2]))\n",
    "    X_trn = reshape_data(globals()['x_'+str(f_mode)+'_trn'], para[0])\n",
    "    X_dev = reshape_data(globals()['x_'+str(f_mode)+'_dev'], para[0])\n",
    "    X_tst = reshape_data(globals()['x_'+str(f_mode)+'_tst'], para[0])\n",
    "    Y_trn = reshape_data(globals()['y_'+str(c_mode)+'_trn'], para[0])\n",
    "    Y_dev = reshape_data(globals()['y_'+str(c_mode)+'_dev'], para[0])\n",
    "    Y_tst = reshape_data(globals()['y_'+str(c_mode)+'_tst'], para[0])\n",
    "    \n",
    "    model = attBLSTM(para[1], para[2], globals()['nb_class_'+str(c_mode)], opt_func)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt_func, metrics=['categorical_accuracy'])\n",
    "    model.fit(X_trn, Y_trn, batch_size=batch_size, epochs=nb_epoch_ab, \n",
    "              validation_split=0.05, callbacks=[early_stopping], verbose=2)\n",
    "    \n",
    "    model_eval(f_ab, c_mode, ab_file_out, model, para[0])\n",
    "    ab_model_f = ab_model_dir\n",
    "    model.save(ab_model_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on s2 and s3 data only\n",
    "\n",
    "# Mode control for feature set and class\n",
    "time_stamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "nb_epoch_no_S1 = 20\n",
    "\n",
    "# output files\n",
    "no_S1_file_out = 'exp4/logs/LSTM_no_S1_' + c_mode + '_sum_' + time_stamp + '.txt'\n",
    "no_S1_model_dir = 'exp4/models/LSTM/' + c_mode + '/no_S1/'\n",
    "with open(no_S1_file_out, 'a') as outfile:\n",
    "    outfile.write('Parameters: features = no_S1, class = %s, time_step = %s, [h1,h2,h3] = %s, attention_width = %s\\n' \n",
    "               % (c_mode, para[0], para[1], para[2]))\n",
    "print('Parameters: features = no_S1, class = %s, time_step = %s, [h1,h2,h3] = %s, attention_width = %s\\n' \n",
    "      % (c_mode, para[0], para[1], para[2]))\n",
    "X_trn = reshape_data(X_no_S1_trn, para[0])\n",
    "X_dev = reshape_data(x_all_dev, para[0])\n",
    "X_tst = reshape_data(x_all_tst, para[0])\n",
    "Y_trn = reshape_data(globals()['y_'+str(c_mode)+'_no_S1_trn'], para[0])\n",
    "Y_dev = reshape_data(globals()['y_'+str(c_mode)+'_dev'], para[0])\n",
    "Y_tst = reshape_data(globals()['y_'+str(c_mode)+'_tst'], para[0])\n",
    "\n",
    "model = attBLSTM(para[1], para[2], globals()['nb_class_'+str(c_mode)], opt_func)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_func, metrics=['categorical_accuracy'])\n",
    "model.fit(X_trn, Y_trn, batch_size=batch_size, epochs=nb_epoch_no_S1, \n",
    "          validation_split=0.05, callbacks=[early_stopping], verbose=2)\n",
    "\n",
    "# print results on training set\n",
    "# model.evaluate(X_trn, Y_trn, batch_size=batch_size)\n",
    "# trn_pred = model.predict(X_trn)\n",
    "# y_trn_non_category = [ np.argmax(t[0]) for t in Y_trn ]\n",
    "# y_trn_predict_non_category = [ np.argmax(t[0]) for t in trn_pred ]\n",
    "# print('Confusion Matrix on train set')\n",
    "# print(confusion_matrix(y_trn_non_category, y_trn_predict_non_category))\n",
    "# trn_f1 = f1_score(y_trn_non_category, y_trn_predict_non_category, average='weighted')\n",
    "# print('Weighted F1-score on train set:', trn_f1)\n",
    "# with open(ab_file_out, 'a') as logfile:\n",
    "    # logfile.write('Confusion Matrix on train set\\n')\n",
    "    # np.savetxt(logfile, confusion_matrix(y_trn_non_category, y_trn_predict_non_category))\n",
    "    # logfile.write('Weighted F1-score on train set: %s' % trn_f1)\n",
    "\n",
    "# print results on dev set\n",
    "model.evaluate(X_dev, Y_dev, batch_size=batch_size)\n",
    "dev_pred = model.predict(X_dev)\n",
    "y_dev_non_category = [ np.argmax(t[0]) for t in Y_dev ]\n",
    "y_dev_predict_non_category = [ np.argmax(t[0]) for t in dev_pred ]\n",
    "print('Confusion Matrix on development set')\n",
    "print(confusion_matrix(y_dev_non_category, y_dev_predict_non_category))\n",
    "dev_f1 = f1_score(y_dev_non_category, y_dev_predict_non_category, average='weighted')\n",
    "print('Weighted F1-score on development set: %s' % dev_f1)\n",
    "with open(no_S1_file_out, 'a') as logfile:\n",
    "    logfile.write('Confusion Matrix on development set\\n')\n",
    "    np.savetxt(logfile, confusion_matrix(y_dev_non_category, y_dev_predict_non_category)) \n",
    "    logfile.write('Weighted F1-score on development set: %s' % dev_f1)\n",
    "\n",
    "\n",
    "# print results on test set\n",
    "model.evaluate(X_tst, Y_tst, batch_size=batch_size)\n",
    "tst_pred = model.predict(X_tst)\n",
    "y_tst_non_category = [ np.argmax(t[0]) for t in Y_tst ]\n",
    "y_tst_predict_non_category = [ np.argmax(t[0]) for t in tst_pred ]\n",
    "print('Confusion Matrix on test set')\n",
    "print(confusion_matrix(y_tst_non_category, y_tst_predict_non_category))\n",
    "tst_f1 = f1_score(y_tst_non_category, y_tst_predict_non_category, average='weighted')\n",
    "print('Weighted F1-score on test set:', tst_f1)\n",
    "with open(no_S1_file_out, 'a') as logfile:\n",
    "    logfile.write('Confusion Matrix on test set\\n')\n",
    "    np.savetxt(logfile, confusion_matrix(y_tst_non_category, y_tst_predict_non_category)) \n",
    "    logfile.write('Weighted F1-score on test set: %s' % tst_f1)\n",
    "\n",
    "#model_eval(f_ab, c_mode, ab_file_out, model, para[0])\n",
    "no_S1_model_f = no_S1_model_dir\n",
    "model.save(no_S1_model_f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
