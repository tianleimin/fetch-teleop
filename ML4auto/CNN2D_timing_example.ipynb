{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zisrGCcZUkdU"
   },
   "outputs": [],
   "source": [
    "# FACT HRC (FACT-support): example ML model implementation\n",
    "# Binary classification on handover timings (when to execute an action primitive):\n",
    "# Predict whether or not a data instance is within 5s before start of episode / arm reaching to participant / arm tucking\n",
    "# Use balanced dataset with randomly sampled not-init time steps\n",
    "\n",
    "# import the required modules\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import datetime\n",
    "import statistics\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "tf.config.experimental.set_visible_devices(devices=gpus[0], device_type='GPU')\n",
    "tf.config.experimental.set_memory_growth(device=gpus[0], enable=True)\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras import Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adamax, SGD, Adam\n",
    "from keras import initializers\n",
    "from keras.metrics import *\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize,LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# turn off the warnings, be careful when use this\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "batch_size = 8 # for estimating error gradient\n",
    "# number of total epochs to train the model\n",
    "nb_epoch = 100\n",
    "# to prevent over-fitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "# label and feature columns in the episodic data\n",
    "feat_cols = 48 # only use keypoints (x,y,z,conf) features\n",
    "feat_cols_end = feat_cols + 100\n",
    "# number of classes\n",
    "nb_class = 2 # {'0', '1'}\n",
    "# 5-fold CV\n",
    "cv_list = ['cv_1.csv', 'cv_2.csv', 'cv_3.csv', 'cv_4.csv', 'cv_5.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape panda.DataFrame to Keras style: (batch_size, time_step, nb_features)\n",
    "def reshape_data(data, n_prev):\n",
    "    docX = []\n",
    "    # add rows of 0s if there are too few rows for time step padding\n",
    "    df0 = pd.DataFrame(0.0, index=np.arange(n_prev), columns=data.columns)\n",
    "    if len(data) < n_prev:\n",
    "        data = pd.concat([data,df0])\n",
    "    # time step padding\n",
    "    for i in range(len(data)):\n",
    "        if i < (len(data)-n_prev):\n",
    "            docX.append(data[i:i+n_prev])\n",
    "        else: # the frames in the last window use the same context\n",
    "            docX.append(data[(len(data)-n_prev):len(data)])\n",
    "    alsX = np.array(docX)\n",
    "    return alsX\n",
    "\n",
    "# one-hot encoding of the class labels\n",
    "def one_hot(labels):\n",
    "    labels_converted = []\n",
    "    for label in labels:\n",
    "        if label == 0:\n",
    "            label_converted = [1,0]\n",
    "        elif label == 1:\n",
    "            label_converted = [0,1]\n",
    "        labels_converted.append(label_converted)\n",
    "    labels_converted = np.asarray(labels_converted)\n",
    "    return labels_converted\n",
    "\n",
    "# construct data\n",
    "def feature_read(df_file, feat_cols, label_col):\n",
    "    # read in data\n",
    "    data = pd.read_csv(df_file, header=0)\n",
    "    \n",
    "    # creating feature set\n",
    "    x_all = data.iloc[:, feat_cols:feat_cols_end]\n",
    "\n",
    "    # creating label arrays\n",
    "    y_all = one_hot(data[label_col])\n",
    "    \n",
    "    return x_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define AlexNet model\n",
    "# Network structure inspired by: https://github.com/eweill/keras-deepcv/blob/master/models/classification/alexnet.py\n",
    "def Alex(X_trn, cnn_size=[128,256], fc_size=512, l1=0.01, nb_class=3):\n",
    "    # Initialize model\n",
    "    alexnet = Sequential()\n",
    "\n",
    "    # Layer 1\n",
    "    alexnet.add(Conv2D(cnn_size[0], (11, 11), input_shape=(X_trn.shape[1], X_trn.shape[2],1),\n",
    "                       padding='same', kernel_regularizer=regularizers.l1_l2(l1=l1, l2=0.01)))\n",
    "    alexnet.add(BatchNormalization())\n",
    "    alexnet.add(Activation('relu'))\n",
    "    alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Layer 2\n",
    "    alexnet.add(Conv2D(cnn_size[0], (5, 5), padding='same'))\n",
    "    alexnet.add(BatchNormalization())\n",
    "    alexnet.add(Activation('relu'))\n",
    "    alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Layer 3\n",
    "    alexnet.add(ZeroPadding2D((1, 1)))\n",
    "    alexnet.add(Conv2D(cnn_size[0], (3, 3), padding='same'))\n",
    "    alexnet.add(BatchNormalization())\n",
    "    alexnet.add(Activation('relu'))\n",
    "    alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Layer 4\n",
    "    alexnet.add(ZeroPadding2D((1, 1)))\n",
    "    alexnet.add(Conv2D(cnn_size[1], (3, 3), padding='same'))\n",
    "    alexnet.add(BatchNormalization())\n",
    "    alexnet.add(Activation('relu'))\n",
    "\n",
    "    # Layer 5\n",
    "    alexnet.add(ZeroPadding2D((1, 1)))\n",
    "    alexnet.add(Conv2D(cnn_size[1], (3, 3), padding='same'))\n",
    "    alexnet.add(BatchNormalization())\n",
    "    alexnet.add(Activation('relu'))\n",
    "    alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Layer 6\n",
    "    alexnet.add(ZeroPadding2D((1, 1)))\n",
    "    alexnet.add(Conv2D(cnn_size[1], (3, 3), padding='same'))\n",
    "    alexnet.add(BatchNormalization())\n",
    "    alexnet.add(Activation('relu'))\n",
    "    alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Layer 7 (Dense)\n",
    "    alexnet.add(Flatten())\n",
    "    alexnet.add(Dense(fc_size))\n",
    "    alexnet.add(BatchNormalization())\n",
    "    alexnet.add(Activation('relu'))\n",
    "    alexnet.add(Dropout(0.5))\n",
    "\n",
    "    # Layer 8 (output)\n",
    "    alexnet.add(Dense(nb_class))\n",
    "    alexnet.add(BatchNormalization())\n",
    "    alexnet.add(Activation('softmax'))\n",
    "\n",
    "    return alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 1D + Dense\n",
    "def CNN1D(X_trn, cnn_size=[128,256], fc_size=512, l1=0.01, nb_class=3):\n",
    "    # Initialize model\n",
    "    cnn = Sequential()\n",
    "    \n",
    "    # input layer\n",
    "    # X_trn.shape = (sampe number, time step, feature dimension)\n",
    "    cnn.add(Input(shape=(X_trn.shape[1], X_trn.shape[2])))\n",
    "    cnn.add(BatchNormalization()) # normalisation layer\n",
    "    cnn.add(Dropout(rate=0.5)) # dropout layer\n",
    "    \n",
    "    # CNN layers\n",
    "    cnn.add(Conv1D(filters=cnn_size[0], kernel_size=3, activation='relu', padding='same'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Conv1D(filters=cnn_size[0], kernel_size=3, activation='relu', padding='same'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Conv1D(filters=cnn_size[0], kernel_size=3, activation='relu', padding='same'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Conv1D(filters=cnn_size[1], kernel_size=3, activation='relu', padding='same'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Conv1D(filters=cnn_size[1], kernel_size=3, activation='relu', padding='same'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Conv1D(filters=cnn_size[1], kernel_size=3, activation='relu', padding='same'))\n",
    "    cnn.add(BatchNormalization())\n",
    "\n",
    "    # Dense layer\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(fc_size))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Dropout(0.5))\n",
    "\n",
    "    # ouptut layer\n",
    "    cnn.add(Dense(nb_class))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Activation('softmax'))\n",
    "\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 1D + LSTM\n",
    "def CNN_LSTM(X_trn, cnn_size=[128,256], fc_size=512, l1=0.01, nb_class=3):\n",
    "    # Initialize model\n",
    "    alexLSTM = Sequential()\n",
    "    \n",
    "    # input layer\n",
    "    # X_trn.shape = (sampe number, time step, feature dimension)\n",
    "    alexLSTM.add(Input(shape=(X_trn.shape[1], X_trn.shape[2], 1)))\n",
    "    alexLSTM.add(BatchNormalization()) # normalisation layer\n",
    "    alexLSTM.add(Dropout(rate=0.5)) # dropout layer\n",
    "    \n",
    "    # CNN layers\n",
    "    alexLSTM.add(TimeDistributed(Conv1D(filters=cnn_size[0], kernel_size=3, activation='relu', padding='same')))\n",
    "    alexLSTM.add(TimeDistributed(BatchNormalization()))\n",
    "    alexLSTM.add(TimeDistributed(Conv1D(filters=cnn_size[0], kernel_size=3, activation='relu', padding='same')))\n",
    "    alexLSTM.add(TimeDistributed(BatchNormalization()))\n",
    "    alexLSTM.add(TimeDistributed(Conv1D(filters=cnn_size[0], kernel_size=3, activation='relu', padding='same')))\n",
    "    alexLSTM.add(TimeDistributed(BatchNormalization()))\n",
    "    alexLSTM.add(TimeDistributed(Conv1D(filters=cnn_size[1], kernel_size=3, activation='relu', padding='same')))\n",
    "    alexLSTM.add(TimeDistributed(BatchNormalization()))\n",
    "    alexLSTM.add(TimeDistributed(Conv1D(filters=cnn_size[1], kernel_size=3, activation='relu', padding='same')))\n",
    "    alexLSTM.add(TimeDistributed(BatchNormalization()))\n",
    "    alexLSTM.add(TimeDistributed(Conv1D(filters=cnn_size[1], kernel_size=3, activation='relu', padding='same')))\n",
    "    alexLSTM.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "    # LSTM layer\n",
    "    alexLSTM.add(TimeDistributed(Flatten()))\n",
    "    alexLSTM.add(LSTM(units=fc_size, kernel_regularizer=regularizers.L1L2(l1=l1,l2=0.0)))\n",
    "    alexLSTM.add(BatchNormalization())\n",
    "    alexLSTM.add(Dropout(0.5))\n",
    "\n",
    "    # ouptut layer\n",
    "    alexLSTM.add(Dense(nb_class))\n",
    "    alexLSTM.add(BatchNormalization())\n",
    "    alexLSTM.add(Activation('softmax'))\n",
    "\n",
    "    return alexLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC + LSTM\n",
    "def FC_LSTM(X_trn, cnn_size=[128,256], fc_size=512, l1=0.01, nb_class=3):\n",
    "    # Initialize model\n",
    "    FClstm = Sequential()\n",
    "    \n",
    "    # input layer\n",
    "    # X_trn.shape = (sampe number, time step, feature dimension)\n",
    "    FClstm.add(Input(shape=(X_trn.shape[1], X_trn.shape[2], 1)))\n",
    "    FClstm.add(BatchNormalization()) # normalisation layer\n",
    "    FClstm.add(Dropout(rate=0.5)) # dropout layer\n",
    "\n",
    "    # Layer 1\n",
    "    FClstm.add(TimeDistributed(Dense(cnn_size[0])))\n",
    "    FClstm.add(TimeDistributed(BatchNormalization()))\n",
    "    FClstm.add(TimeDistributed(Activation('relu')))\n",
    "\n",
    "    # Layer 2\n",
    "    FClstm.add(TimeDistributed(Dense(cnn_size[0])))\n",
    "    FClstm.add(TimeDistributed(BatchNormalization()))\n",
    "    FClstm.add(TimeDistributed(Activation('relu')))\n",
    "\n",
    "    # Layer 3\n",
    "    FClstm.add(TimeDistributed(Dense(cnn_size[0])))\n",
    "    FClstm.add(TimeDistributed(BatchNormalization()))\n",
    "    FClstm.add(TimeDistributed(Activation('relu')))\n",
    "\n",
    "    # Layer 4\n",
    "    FClstm.add(TimeDistributed(Dense(cnn_size[1])))\n",
    "    FClstm.add(TimeDistributed(BatchNormalization()))\n",
    "    FClstm.add(TimeDistributed(Activation('relu')))\n",
    "\n",
    "    # Layer 5\n",
    "    FClstm.add(TimeDistributed(Dense(cnn_size[1])))\n",
    "    FClstm.add(TimeDistributed(BatchNormalization()))\n",
    "    FClstm.add(TimeDistributed(Activation('relu')))\n",
    "\n",
    "    # Layer 6\n",
    "    FClstm.add(TimeDistributed(Dense(cnn_size[1])))\n",
    "    FClstm.add(TimeDistributed(BatchNormalization()))\n",
    "    FClstm.add(TimeDistributed(Activation('relu')))\n",
    "    FClstm.add(TimeDistributed(Dropout(0.5)))\n",
    "\n",
    "    # Layer 7 (LSTM)\n",
    "    FClstm.add(TimeDistributed(Flatten()))\n",
    "    FClstm.add(LSTM(units=fc_size, kernel_regularizer=regularizers.L1L2(l1=l1,l2=0.0)))\n",
    "    FClstm.add(BatchNormalization())\n",
    "    FClstm.add(Dropout(0.5))\n",
    "\n",
    "    # Layer 8 (output)\n",
    "    FClstm.add(Dense(nb_class))\n",
    "    FClstm.add(BatchNormalization())\n",
    "    FClstm.add(Activation('softmax'))\n",
    "\n",
    "    return FClstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use fully connected dense layers instead of CNN2D in AlexNet structure\n",
    "def FC_Alex(X_trn, cnn_size=[128,256], fc_size=512, l1=0.01, nb_class=3):\n",
    "    # Initialize model\n",
    "    FCnet = Sequential()\n",
    "\n",
    "    # Layer 1\n",
    "    FCnet.add(Dense(cnn_size[0], input_shape=[X_trn.shape[1]], kernel_regularizer=regularizers.l1_l2(l1=l1, l2=0.01)))\n",
    "    FCnet.add(BatchNormalization())\n",
    "    FCnet.add(Activation('relu'))\n",
    "\n",
    "    # Layer 2\n",
    "    FCnet.add(Dense(cnn_size[0]))\n",
    "    FCnet.add(BatchNormalization())\n",
    "    FCnet.add(Activation('relu'))\n",
    "\n",
    "    # Layer 3\n",
    "    FCnet.add(Dense(cnn_size[0]))\n",
    "    FCnet.add(BatchNormalization())\n",
    "    FCnet.add(Activation('relu'))\n",
    "\n",
    "    # Layer 4\n",
    "    FCnet.add(Dense(cnn_size[1]))\n",
    "    FCnet.add(BatchNormalization())\n",
    "    FCnet.add(Activation('relu'))\n",
    "\n",
    "    # Layer 5\n",
    "    FCnet.add(Dense(cnn_size[1]))\n",
    "    FCnet.add(BatchNormalization())\n",
    "    FCnet.add(Activation('relu'))\n",
    "\n",
    "    # Layer 6\n",
    "    FCnet.add(Dense(cnn_size[1]))\n",
    "    FCnet.add(BatchNormalization())\n",
    "    FCnet.add(Activation('relu'))\n",
    "    FCnet.add(Dropout(0.5))\n",
    "\n",
    "    # Layer 7 (Dense)\n",
    "    FCnet.add(Dense(fc_size))\n",
    "    FCnet.add(BatchNormalization())\n",
    "    FCnet.add(Activation('relu'))\n",
    "    FCnet.add(Dropout(0.5))\n",
    "\n",
    "    # Layer 8 (output)\n",
    "    FCnet.add(Dense(nb_class))\n",
    "    FCnet.add(BatchNormalization())\n",
    "    FCnet.add(Activation('softmax'))\n",
    "\n",
    "    return FCnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model performance and print results\n",
    "def model_eval(model, X_tst, Y_tst, log_f, batch_size, label_time_pad=False):\n",
    "    # evaluate model on testing set\n",
    "    loss, acc = model.evaluate(X_tst, Y_tst, batch_size=batch_size, verbose=1)\n",
    "    y_tst_pred = model.predict(X_tst)\n",
    "    \n",
    "    # get confusion matrix and metrics\n",
    "    # when the labels are time-padded\n",
    "    if label_time_pad:\n",
    "        y_tst_non_category = [ np.argmax(t[0]) for t in Y_tst ]\n",
    "        y_tst_predict_non_category = [ np.argmax(t[0]) for t in y_tst_pred ]\n",
    "    else:\n",
    "        y_tst_non_category = np.argmax(Y_tst, axis=1)\n",
    "        y_tst_predict_non_category = np.argmax(y_tst_pred, axis=1)\n",
    "    \n",
    "    print('\\nConfusion Matrix on test set')\n",
    "    print(confusion_matrix(y_tst_non_category, y_tst_predict_non_category).astype(int))\n",
    "    tst_f1 = f1_score(y_tst_non_category, y_tst_predict_non_category, average='weighted')\n",
    "    tst_f1_unweighted = f1_score(y_tst_non_category, y_tst_predict_non_category, average='macro')\n",
    "    print(f'\\nWeighted F1-score on test set = {tst_f1:.3%}')\n",
    "    print(f'\\nUnweighted F1-score on test set = {tst_f1_unweighted:.3%}')\n",
    "    print(f'\\naccuracy on test set = {acc:.3%}')\n",
    "    \n",
    "    # print to log files\n",
    "    with open(log_f, 'a') as logfile:\n",
    "        logfile.write('Confusion Matrix on test set\\n')\n",
    "        logfile.write(str(confusion_matrix(y_tst_non_category, y_tst_predict_non_category).astype(int)))\n",
    "        logfile.write(f'\\nWeighted F1-score on test set = {tst_f1:.3%}')\n",
    "        logfile.write(f'\\nUnweighted F1-score on test set = {tst_f1_unweighted:.3%}')\n",
    "        logfile.write(f'\\naccuracy on test set = {acc:.3%}')\n",
    "    \n",
    "    return tst_f1, tst_f1_unweighted, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best parameter set\n",
    "def GS_summary(cv_list, para_list):\n",
    "    # find the highest score\n",
    "    best = cv_list.index(max(cv_list))\n",
    "    best_count = best + 1\n",
    "    result = cv_list[best]\n",
    "    para = para_list[best]\n",
    "    return best_count, result, para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search training with 5-fold CV\n",
    "def Training_GS_CV(model_s, c_mode_trn, c_mode_tst, para_list, total, file_log, model_name, cv_list, cb=True, save_model=False):\n",
    "    # lists to store metrics\n",
    "    f1_cv_list_weighted = []\n",
    "    f1_cv_list_unweighted = []\n",
    "    acc_cv_list = []\n",
    "    count = 1\n",
    "\n",
    "    # print data configuration\n",
    "    print(f'========= Model: {model_s}; Data config: {c_mode_trn} as trn and {c_mode_tst} as tst =========\\n')\n",
    "    with open(file_log, 'a') as outfile:\n",
    "        outfile.write(f'========= Model: {model_s}; Data config: {c_mode_trn} as trn and {c_mode_tst} as tst =========\\n')\n",
    "    \n",
    "    # grid search with given parameter list\n",
    "    for para in para_list:\n",
    "        # parameters being used in this iteration\n",
    "        time_step = para[0]\n",
    "        cnn_size = para[1]\n",
    "        fc_size = para[2]\n",
    "        l1 = para[3]\n",
    "        lr = para[4]\n",
    "\n",
    "        # print information about the run\n",
    "        print(f'\\n------------------ No. {count} of {total} ------------------')\n",
    "        print(f'\\nHyperparameters: l1_weight = {l1}, learning_rate = {lr}')\n",
    "        print(f'\\nParameters: time_step = {time_step}, cnn_size = {cnn_size}, fc_size = {fc_size}')\n",
    "        with open(file_log, 'a') as outfile:\n",
    "            outfile.write(f'\\n------------------ No. {count} of {total} ------------------')\n",
    "            outfile.write(f'\\nHyperparameters: l1_weight = {l1}, learning_rate = {lr}')\n",
    "            outfile.write(f'\\nParameters: time_step = {time_step}, cnn_size = {cnn_size}, fc_size = {fc_size}')\n",
    "\n",
    "        # 5-fold cross-validation\n",
    "        # for computing cv average\n",
    "        f1_weighted = []\n",
    "        f1_unweighted = []\n",
    "        acc = []\n",
    "        f1_weighted_avg = 0.0\n",
    "        f1_unweighted_avg = 0.0\n",
    "        acc_avg = 0.0\n",
    "        f1_weighted_std = 0.0\n",
    "        f1_unweighted_std = 0.0\n",
    "        acc_std = 0.0\n",
    "\n",
    "        # loop over the 5 folds\n",
    "        for cv in cv_list:\n",
    "            # input data files\n",
    "            df_file_trn = 'data/ML/combined_' + c_mode_trn + '_ML_trn_' + cv\n",
    "            df_file_tst = 'data/ML/combined_' + c_mode_tst + '_ML_tst_' + cv\n",
    "\n",
    "            # information about the cross-validation\n",
    "            print(f'\\n=== fold: {cv} ===\\n')\n",
    "            with open(file_log, 'a') as outfile:\n",
    "                outfile.write(f'\\n=== fold: {cv} ===\\n')\n",
    "\n",
    "            # read in data\n",
    "            x_all_trn, y_all_trn = feature_read(df_file_trn, feat_cols, label_col)\n",
    "            x_all_tst, y_all_tst = feature_read(df_file_tst, feat_cols, label_col)\n",
    "\n",
    "            # building selected model\n",
    "            if model_s == 'Alex':\n",
    "                # time step padding\n",
    "                X_trn = reshape_data(x_all_trn, time_step)\n",
    "                X_tst = reshape_data(x_all_tst, time_step)\n",
    "                # reshape data for Conv2D: https://stackoverflow.com/a/43897173\n",
    "                X_trn = X_trn.reshape(X_trn.shape[0],X_trn.shape[1],X_trn.shape[2],1)\n",
    "                X_tst = X_tst.reshape(X_tst.shape[0],X_tst.shape[1],X_tst.shape[2],1)\n",
    "                # no time step padding for labels\n",
    "                Y_trn = y_all_trn\n",
    "                Y_tst = y_all_tst\n",
    "                label_time_pad = False\n",
    "                # build model\n",
    "                model = Alex(X_trn, cnn_size, fc_size, l1, nb_class)\n",
    "            elif model_s == 'CNN1D':\n",
    "                # time step padding for LSTM\n",
    "                X_trn = reshape_data(x_all_trn, time_step)\n",
    "                X_tst = reshape_data(x_all_tst, time_step)\n",
    "                Y_trn = y_all_trn\n",
    "                Y_tst = y_all_tst\n",
    "                label_time_pad = False\n",
    "                # build model\n",
    "                model = CNN1D(X_trn, cnn_size, fc_size, l1, nb_class)\n",
    "            elif model_s == 'CNN_LSTM':\n",
    "                # time step padding for LSTM\n",
    "                X_trn = reshape_data(x_all_trn, time_step)\n",
    "                X_tst = reshape_data(x_all_tst, time_step)\n",
    "                Y_trn = y_all_trn\n",
    "                Y_tst = y_all_tst\n",
    "                label_time_pad = False\n",
    "                # build model\n",
    "                model = CNN_LSTM(X_trn, cnn_size, fc_size, l1, nb_class)\n",
    "            elif model_s == 'FC_LSTM':\n",
    "                # time step padding for LSTM\n",
    "                X_trn = reshape_data(x_all_trn, time_step)\n",
    "                X_tst = reshape_data(x_all_tst, time_step)\n",
    "                Y_trn = y_all_trn\n",
    "                Y_tst = y_all_tst\n",
    "                label_time_pad = False\n",
    "                # build model\n",
    "                model = FC_LSTM(X_trn, cnn_size, fc_size, l1, nb_class)\n",
    "            elif model_s == 'FC_Alex':\n",
    "                # no reshaping in fully connected\n",
    "                X_trn = x_all_trn\n",
    "                X_tst = x_all_tst\n",
    "                Y_trn = y_all_trn\n",
    "                Y_tst = y_all_tst\n",
    "                label_time_pad = False\n",
    "                model = FC_Alex(X_trn, cnn_size, fc_size, l1, nb_class)\n",
    "            else:\n",
    "                print('Unknown model!')\n",
    "\n",
    "            # training\n",
    "            opt_func = Adam(learning_rate=lr)\n",
    "            model.compile(optimizer=opt_func, loss='binary_crossentropy', metrics=['accuracy']) # binary classification\n",
    "            model.summary()\n",
    "            with open(file_log, 'a') as outfile:\n",
    "                model.summary(print_fn=lambda x: outfile.write(x + '\\n'))\n",
    "            if cb: # with early stopping callback\n",
    "                hist = model.fit(X_trn, Y_trn, batch_size=batch_size, epochs=nb_epoch, \n",
    "                                 callbacks=[early_stopping], verbose=1, validation_data=(X_tst, Y_tst))\n",
    "            else: # no early stopping\n",
    "                hist = model.fit(X_trn, Y_trn, batch_size=batch_size, epochs=nb_epoch, \n",
    "                                 verbose=1, validation_data=(X_tst, Y_tst))\n",
    "\n",
    "\n",
    "            # evaluation\n",
    "            tst_f1, tst_f1_unweighted, tst_acc = model_eval(model, X_tst, Y_tst, file_log, batch_size, label_time_pad)\n",
    "            f1_weighted.append(tst_f1)\n",
    "            f1_unweighted.append(tst_f1_unweighted)\n",
    "            acc.append(tst_acc)\n",
    "            # saving model.h5 or not\n",
    "            if save_model:\n",
    "                model.save(model_name)\n",
    "                \n",
    "\n",
    "            # visualise the training\n",
    "            fig = plt.figure()\n",
    "            plt.subplot(2,1,1)\n",
    "            plt.plot(hist.history['accuracy'])\n",
    "            plt.plot(hist.history['val_accuracy'])\n",
    "            plt.title('model accuracy')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='lower right')\n",
    "\n",
    "            plt.subplot(2,1,2)\n",
    "            plt.plot(hist.history['loss'])\n",
    "            plt.plot(hist.history['val_loss'])\n",
    "            plt.title('model loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        if len(cv_list) > 1: # running CV\n",
    "            # calculate CV average and std\n",
    "            f1_weighted_avg = statistics.mean(f1_weighted)\n",
    "            f1_unweighted_avg = statistics.mean(f1_unweighted)\n",
    "            acc_avg = statistics.mean(acc)\n",
    "            f1_weighted_std = statistics.stdev(f1_weighted)\n",
    "            f1_unweighted_std = statistics.stdev(f1_unweighted)\n",
    "            acc_std = statistics.stdev(acc)\n",
    "            print(f'\\n*** CV Summary (No. {count} of {total}) ***\\n')\n",
    "            print(f'\\nHyperparameters: batch_size = {batch_size}, learning_rate = {lr}')\n",
    "            print(f'\\nParameters: time_step = {time_step}, cnn_size = {cnn_size}, fc_size = {fc_size}')\n",
    "            print(\n",
    "            f'\\nMean and std of all metrics for predicting {label_col}:\\n \\\n",
    "            Accuracy.avg = {acc_avg:.3%}, Accuracy.std = {acc_std:.3%}\\n \\\n",
    "            F1_weighted.avg = {f1_weighted_avg:.3%}, F1_weighted.std = {f1_weighted_std:.3%}\\n \\\n",
    "            F1_unweighted.avg = {f1_unweighted_avg:.3%}, F1_unweighted.std = {f1_unweighted_std:.3%}\\n')\n",
    "            print('------------------------------------\\n\\n')\n",
    "            with open(file_log, 'a') as outfile:\n",
    "                outfile.write(f'\\n*** CV Summary (No. {count} of {total}) ***\\n')\n",
    "                outfile.write(f'\\nHyperparameters: batch_size = {batch_size}, learning_rate = {lr}')\n",
    "                outfile.write(f'\\nParameters: time_step = {time_step}, cnn_size = {cnn_size}, fc_size = {fc_size}')\n",
    "                outfile.write(\n",
    "                f'\\nMean and std of all metrics for predicting {label_col}:\\n \\\n",
    "                Accuracy.avg = {acc_avg:.3%}, Accuracy.std = {acc_std:.3%}\\n \\\n",
    "                F1_weighted.avg = {f1_weighted_avg:.3%}, F1_weighted.std = {f1_weighted_std:.3%}\\n \\\n",
    "                F1_unweighted.avg = {f1_unweighted_avg:.3%}, F1_unweighted.std = {f1_unweighted_std:.3%}\\n')\n",
    "                outfile.write('------------------------------------\\n\\n')\n",
    "        else: # not running cv (e.g., train-test split for once) then don't compute cv mean and std\n",
    "            f1_weighted_avg = f1_weighted[0]\n",
    "            f1_unweighted_avg = f1_unweighted[0]\n",
    "            acc_avg = acc[0]\n",
    "            print(f'\\n*** CV Summary (No. {count} of {total}) ***\\n')\n",
    "            print(f'\\nHyperparameters: batch_size = {batch_size}, learning_rate = {lr}')\n",
    "            print(f'\\nParameters: time_step = {time_step}, cnn_size = {cnn_size}, fc_size = {fc_size}')\n",
    "            print(\n",
    "            f'\\nAll metrics for predicting {label_col}:\\n \\\n",
    "            Accuracy = {acc_avg:.3%}, F1_weighted = {f1_weighted_avg:.3%}, F1_unweighted = {f1_unweighted_avg:.3%}\\n')\n",
    "            print('------------------------------------\\n\\n')\n",
    "            with open(file_log, 'a') as outfile:\n",
    "                outfile.write(f'\\n*** CV Summary (No. {count} of {total}) ***\\n')\n",
    "                outfile.write(f'\\nHyperparameters: batch_size = {batch_size}, learning_rate = {lr}')\n",
    "                outfile.write(f'\\nParameters: time_step = {time_step}, cnn_size = {cnn_size}, fc_size = {fc_size}')\n",
    "                outfile.write(\n",
    "                f'\\nAll metrics for predicting {label_col}:\\n \\\n",
    "                Accuracy = {acc_avg:.3%}, F1_weighted = {f1_weighted_avg:.3%}, F1_unweighted = {f1_unweighted_avg:.3%}\\n')\n",
    "                outfile.write('------------------------------------\\n\\n')\n",
    "\n",
    "        f1_cv_list_weighted.append(f1_weighted_avg)\n",
    "        f1_cv_list_unweighted.append(f1_unweighted_avg)\n",
    "        acc_cv_list.append(acc_avg)\n",
    "        count = count + 1\n",
    "\n",
    "    return f1_cv_list_weighted, f1_cv_list_unweighted, acc_cv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lists of available models and classification tasks\n",
    "model_s_list = ['Alex', 'CNN_LSTM', 'CNN1D', 'FC_LSTM', 'FC_Alex']\n",
    "classifier_list = ['arm', 'base', 'fin']\n",
    "\n",
    "# train all models in a loooooong loop\n",
    "for classifier in classifier_list:\n",
    "    for model_s in model_s_list:\n",
    "        # label column to use\n",
    "        label_col = classifier + ' init'\n",
    "        # datasets to use\n",
    "        c_mode_trn = 'ep_fold_' + classifier # specify training data\n",
    "        c_mode_tst = 'ep_fold_' + classifier # specify testing data\n",
    "        \n",
    "        cb = True # use early stopping in training\n",
    "        save_model = False # don't save the model\n",
    "\n",
    "        # timestamp\n",
    "        time_stamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "        # parameters to be investigated in grid seearch\n",
    "        time_steps = [5]  # list of input history lengths to test\n",
    "        cnn_sizes = [[8,16]] # CNN filter sizes\n",
    "        fc_sizes = [8] # FC layer sizes\n",
    "        l1s = [0.01] # l1 regularizer weights\n",
    "        lrs = [0.0001] # optimizer learning rates\n",
    "\n",
    "        total = len(time_steps) * len(cnn_sizes) * len(fc_sizes) * len(l1s) * len(lrs) # total number of combos\n",
    "        para_list = list(itertools.product(time_steps, cnn_sizes, fc_sizes, l1s, lrs))\n",
    "\n",
    "        run_id = 'exp/' + model_s + '_' + c_mode_trn + '_' + time_stamp\n",
    "        file_log = run_id + '_log.txt'\n",
    "        model_name = run_id + '_model.h5'\n",
    "        \n",
    "        # run main loop to perform 5-fold CV on the selected classification task using the selected model\n",
    "        f1_cv_list_weighted, f1_cv_list_unweighted, acc_cv_list = Training_GS_CV(model_s, c_mode_trn, c_mode_tst, para_list, \n",
    "                                                                                 total, file_log, model_name, cv_list, \n",
    "                                                                                 cb, save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
