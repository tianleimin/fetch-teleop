{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess collected csv data to use in RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read csv to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data type of columns\n",
    "int_cols = ['episode']\n",
    "cat_cols = ['status', 'handover quality', 'handover type', 'arm status', 'base status', 'handover status']\n",
    "\n",
    "# load raw data\n",
    "participants = ['p1_2022-07-25', 'p2_2022-08-03', 'p3_2022-08-04', 'p4_2022-08-10', 'p5_2022-08-12', \n",
    "                'p6_2022-08-15', 'p7_2022-08-15', 'p8_2022-08-19', 'p9_2022-08-22', 'p10_2022-08-26', \n",
    "                'p11_2022-08-29', 'p12_2022-08-29', 'p13_2022-08-29', 'p14_2022-08-31', 'p15_2022-09-02', \n",
    "                'p16_2022-09-05', 'p17_2022-09-06', 'p18_2022-09-07', 'p19_2022-09-08', 'p20_2022-09-09']\n",
    "#for participant in participants:\n",
    "#    raw_data = 'data/ML/' + participant\n",
    "raw_data = 'data/ML/' + participants[0]\n",
    "\n",
    "# read csv into dataframe\n",
    "df = pd.read_csv((raw_data + '.csv'), header = 0)\n",
    "# specific data type of columns\n",
    "for column in df:\n",
    "    if column in int_cols:\n",
    "        df[column] = df[column].astype('int')\n",
    "    elif column in cat_cols:\n",
    "        df[column] = df[column].astype('category')\n",
    "    else:\n",
    "        df[column] = df[column].astype('float')\n",
    "\n",
    "# fill missing values in the manually entered handover quality and type columns\n",
    "df['handover quality'] = df['handover quality'].cat.add_categories('NEUTRAL')\n",
    "df['handover quality'].fillna('NEUTRAL', inplace =True)\n",
    "#df['handover type'] = df['handover type'].cat.add_categories('NEITHER')\n",
    "#df['handover type'].fillna('NEITHER', inplace =True) \n",
    "df['handover type'].fillna('ROBOT TO HUMAN', inplace =True) \n",
    "\n",
    "# df['handover quality'].dtype\n",
    "# df['handover type'][500:550]\n",
    "\n",
    "# preview\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute timing related stats\n",
    "# # get step index within each episode\n",
    "# df['step'] = np.arange(len(df))\n",
    "# ep_start_step = df['step'][(np.diff(df['episode'].values, prepend=-1) > 0)].values\n",
    "# df['offset'] = ep_start_step[df['episode'].values]\n",
    "# df.loc[:,'step'] = df['step'] - df['offset'] + 1\n",
    "# # get episode length\n",
    "# max_step_by_episode = df.groupby('episode')['step'].max().values\n",
    "# # average episode length (s) per stage\n",
    "# #print(np.mean(max_step_by_episode[:4])*0.1)\n",
    "# #print(np.mean(max_step_by_episode[4:13])*0.1)\n",
    "# #print(np.mean(max_step_by_episode[13:])*0.1)\n",
    "\n",
    "# pause_work_duration = []\n",
    "# pause_storage_duration = []\n",
    "# for g, data in df.groupby('episode'):\n",
    "#     # pause_storage = df1[df['arm status']=='STATIONARY'][df['base status']=='STATIONARY'] \\\n",
    "#     #             [df['status'].isin(['TO PARTICIPANT', 'ROTATE TO PARTICIPANT', 'OPERATOR HANDOVER'])]\n",
    "#     # pause_work = df1[df['arm status']=='STATIONARY'][df['base status']=='STATIONARY'] \\\n",
    "#     #             [df['status'].isin(['TO OPERATOR', 'ROTATE TO OPERATOR', 'PARTICIPANT HANDOVER'])]\n",
    "#     pause_storage = data[df['arm status']=='STATIONARY'][df['base status']=='STATIONARY'] \\\n",
    "#                 [df['status'].isin(['ROTATE TO PARTICIPANT', 'OPERATOR HANDOVER'])]\n",
    "#     pause_work = data[df['arm status']=='STATIONARY'][df['base status']=='STATIONARY'] \\\n",
    "#                 [df['status'].isin(['ROTATE TO OPERATOR', 'PARTICIPANT HANDOVER'])]\n",
    "\n",
    "#     pause_work_duration.append(len(pause_work))\n",
    "#     pause_storage_duration.append(len(pause_storage))\n",
    "    \n",
    "# #print(pause_work_duration)\n",
    "# print(np.mean(pause_work_duration[:4])*0.1)\n",
    "# print(np.mean(pause_work_duration[4:13])*0.1)\n",
    "# print(np.mean(pause_work_duration[13:])*0.1)\n",
    "# #print(pause_storage_duration)\n",
    "# print(np.mean(pause_storage_duration[:4])*0.1)\n",
    "# print(np.mean(pause_storage_duration[4:13])*0.1)\n",
    "# print(np.mean(pause_storage_duration[13:])*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### count how many of each type of handover is in a session\n",
    "# # get step index within each episode\n",
    "# df['step'] = np.arange(len(df))\n",
    "# ep_start_step = df['step'][(np.diff(df['episode'].values, prepend=-1) > 0)].values\n",
    "# df_ep = df['handover status'][ep_start_step]\n",
    "# df_ep\n",
    "# #df_ep.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalise categorical emotion values and replace Fetch predictions when not facing participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise categorical emotion prediction values\n",
    "\n",
    "# categorical emotion values are network weights at output layer\n",
    "# the category label shown in GUI visualisation is the argmax of all 8 classes' weights (e.g., argmax(-8,-3) is -3)\n",
    "\n",
    "# normalise categorical emotion values to [0,1] by (60+x)/60 as the min is 60\n",
    "df.iloc[:,28:36] = df.iloc[:,28:36].apply(lambda x: (60+x)/ 60, axis=0) # OAK-D emotions\n",
    "df.iloc[:,38:46] = df.iloc[:,38:46].apply(lambda x: (60+x)/ 60, axis=0) # Fetch emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace Fetch predictions with OAK-D predictions when it's not facing the participant\n",
    "\n",
    "# use status label to find when the robot is facing the participant\n",
    "facing_participant = ['TO PARTICIPANT', 'PARTICIPANT HANDOVER']\n",
    "not_facing_participant = ['TO OPERATOR', 'ROTATE TO OPERATOR', 'ROTATE TO PARTICIPANT', 'OPERATOR HANDOVER']\n",
    "emo_list = ['neutral','happy','sad','surprise','fear','disgust','anger','contempt','valence','arousal']\n",
    "\n",
    "#df1 = df.iloc[30020:30070]\n",
    "\n",
    "# replace Fetch emotions with OAK-D emotions\n",
    "for index, row in df.iterrows():\n",
    "    if row['status'] in not_facing_participant:\n",
    "        for emo in emo_list:\n",
    "            df.loc[index,str(emo+' (fetch)')] = df.loc[index,str(emo+' (global)')]\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. State observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state observations are OAK-D pose estimations in columns [48:148] and an added task progress (numerical)\n",
    "df['task progress'] = df['episode'] / max(df['episode'])\n",
    "\n",
    "# state observation in a new dataframe\n",
    "df_state = df.iloc[:,48:149]\n",
    "df_state_emo = df.iloc[:,28:149] # state with emotion in there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Example action coding (OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions are categorical values in the 'arm status' and 'handover status' columns\n",
    "# 'base status' excluded for simplicity\n",
    "#df_action = df[['arm status', 'handover status']]\n",
    "\n",
    "# create a new column of action pairs\n",
    "df['action pair'] = df['arm status'].astype('str') + '+' + df['handover status'].astype('str')\n",
    "df['action pair'] = df['action pair'].astype('category')\n",
    "\n",
    "# coded actions\n",
    "action_encoding = {\n",
    "    'STATIONARY+MIDDLE' : 0,\n",
    "    'REACHING+MIDDLE' : 1,\n",
    "    'TUCKING+MIDDLE' : 2,\n",
    "    'STATIONARY+LEFT' : 3,\n",
    "    'REACHING+LEFT' : 4,\n",
    "    'TUCKING+LEFT' : 5,\n",
    "    'STATIONARY+RIGHT' : 6,\n",
    "    'REACHING+RIGHT' : 7,\n",
    "    'TUCKING+RIGHT' : 8,\n",
    "}\n",
    "\n",
    "df['action pair ID'] = df['action pair'].map(action_encoding)\n",
    "df['action pair ID'] = df['action pair ID'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Example reward design (OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotional reward\n",
    "\n",
    "# OAK-D categorical emotion sum\n",
    "df['emotion (global)'] = (df['neutral (global)'] + df['happy (global)'])/2 \\\n",
    "                         - (df['sad (global)'] + df['surprise (global)'] + df['fear (global)'] \\\n",
    "                         + df['disgust (global)'] + df['anger (global)'] + df['contempt (global)'])/6\n",
    "# Fetch categorical emotion sum\n",
    "df['emotion (fetch)'] = (df['neutral (fetch)'] + df['happy (fetch)'])/2 \\\n",
    "                         - (df['sad (fetch)'] + df['surprise (fetch)'] + df['fear (fetch)'] \\\n",
    "                         + df['disgust (fetch)'] + df['anger (fetch)'] + df['contempt (fetch)'])/6\n",
    "\n",
    "# combined emotional reward\n",
    "df['emotional reward'] = (df['emotion (global)'] + df['arousal (global)'] + df['valence (global)'] \\\n",
    "                        + df['emotion (fetch)'] + df['arousal (fetch)'] + df['valence (fetch)'])/6\n",
    "\n",
    "#print(max(df['emotional reward']))\n",
    "#print(min(df['emotional reward']))\n",
    "#print(df['emotional reward'].mean())\n",
    "#print(df['emotional reward'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handover quality reward\n",
    "# replacing categorical labels with values\n",
    "df['handover reward'] = df['handover quality'].map({'GOOD':1.0, 'BAD':-1.0, 'NEUTRAL':0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# combined reward\n",
    "df['combined reward'] = df['emotional reward'] + df['handover reward'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined reward but the handover quality is treated as an episode reward\n",
    "\n",
    "# get step index within each episode\n",
    "df['step'] = np.arange(len(df))\n",
    "ep_start_step = df['step'][(np.diff(df['episode'].values, prepend=-1) > 0)].values\n",
    "df['offset'] = ep_start_step[df['episode'].values]\n",
    "df.loc[:,'step'] = df['step'] - df['offset'] + 1\n",
    "# create a step progress fraction within each episode\n",
    "max_step_by_episode = df.groupby('episode')['step'].max().values\n",
    "df['fraction'] = df['step'] / np.repeat(max_step_by_episode, max_step_by_episode)\n",
    "# add handover quality reward scaled by step progress fraction so that later steps get more of the episodic reward\n",
    "df['combined reward async'] = df['emotional reward'] + df['handover reward'].astype('float')*df['fraction']\n",
    "#df[df['episode']==3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Only keeping columns needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keeping columns relevant\n",
    "df_time = df[['time (s)', 'episode', 'step']]\n",
    "df_tidy = pd.concat([df_time, df_state, df['action pair ID'], df['combined reward async']], axis=1)\n",
    "df_minerva = pd.concat([df['episode'], df_state, df['action pair ID'], df['combined reward async']], axis=1)\n",
    "df_ML = pd.concat([df_time, df_state_emo, df['handover reward'], df['emotional reward'], df['combined reward async'], df['base status'], df['arm status'], df['handover status'], df['status']], axis=1)\n",
    "#df_ML[df['episode']==3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use data segments when the robot is at work area to simplify learning (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State machine for overall robot status:\n",
    "# To participant -> Participant handover -> Rotate to operator ->\n",
    "# To operator -> Operator handover -> Rotate to participant\n",
    "\n",
    "# extract segments\n",
    "df_seg_otp = df_ML.loc[df_ML['status'].isin(['TO PARTICIPANT', 'PARTICIPANT HANDOVER'])]\n",
    "df_seg_arm = df_ML.loc[df_ML['status']=='PARTICIPANT HANDOVER']\n",
    "\n",
    "#print(df_seg['arm status'].value_counts())\n",
    "#print(df_seg['base status'].value_counts())\n",
    "#print(df_seg['handover quality'].value_counts())\n",
    "#print(df_seg['action pair ID'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # only keeping columns relevant\n",
    "# df_seg_time = df_seg[['time (s)', 'episode', 'step']]\n",
    "# df_seg_state = df_seg.iloc[:,48:149]\n",
    "# df_seg_action = df_seg['action pair ID']\n",
    "# df_seg_reward = df_seg['combined reward async']\n",
    "# df_seg_tidy = pd.concat([df_seg_time, df_seg_state, df_seg_action, df_seg_reward], axis=1)\n",
    "# df_seg_minerva = pd.concat([df_seg['episode'], df_seg_state, df_seg_action, df_seg_reward], axis=1)\n",
    "\n",
    "# #df_seg_tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Save preprocessed data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed_f = raw_data + '_processed' + '.csv'\n",
    "#df_tidy.to_csv(processed_f, index=False)\n",
    "#df_seg_tidy.to_csv(processed_f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in the format of Minerva UI\n",
    "df_ML.to_csv((raw_data + '_processed_base.csv'), index=False)\n",
    "df_seg_otp.to_csv((raw_data + '_processed_otp.csv'), index=False)\n",
    "df_seg_arm.to_csv((raw_data + '_processed_arm.csv'), index=False)\n",
    "#new_header = ['episode','observation:0','observation:1','observation:2','observation:3','observation:4','observation:5','observation:6','observation:7','observation:8','observation:9','observation:10','observation:11','observation:12','observation:13','observation:14','observation:15','observation:16','observation:17','observation:18','observation:19','observation:20','observation:21','observation:22','observation:23','observation:24','observation:25','observation:26','observation:27','observation:28','observation:29','observation:30','observation:31','observation:32','observation:33','observation:34','observation:35','observation:36','observation:37','observation:38','observation:39','observation:40','observation:41','observation:42','observation:43','observation:44','observation:45','observation:46','observation:47','observation:48','observation:49','observation:50','observation:51','observation:52','observation:53','observation:54','observation:55','observation:56','observation:57','observation:58','observation:59','observation:60','observation:61','observation:62','observation:63','observation:64','observation:65','observation:66','observation:67','observation:68','observation:69','observation:70','observation:71','observation:72','observation:73','observation:74','observation:75','observation:76','observation:77','observation:78','observation:79','observation:80','observation:81','observation:82','observation:83','observation:84','observation:85','observation:86','observation:87','observation:88','observation:89','observation:90','observation:91','observation:92','observation:93','observation:94','observation:95','observation:96','observation:97','observation:98','observation:99','observation:100','action:0','reward']\n",
    "#df_seg_minerva.columns = new_header\n",
    "#df_seg_minerva.to_csv(processed_minerva_f, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Concatenate individually preprocessed csv files for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# participants = ['p1_2022-07-25', 'p2_2022-08-03', 'p3_2022-08-04', 'p4_2022-08-10', 'p5_2022-08-12', \n",
    "#                 'p6_2022-08-15', 'p7_2022-08-15', 'p8_2022-08-19', 'p9_2022-08-22', 'p10_2022-08-26', \n",
    "#                 'p11_2022-08-29', 'p12_2022-08-29', 'p13_2022-08-29', 'p14_2022-08-31', 'p15_2022-09-02', \n",
    "#                 'p16_2022-09-05', 'p17_2022-09-06', 'p18_2022-09-07', 'p19_2022-09-08', 'p20_2022-09-09']\n",
    "# # tranining, development, and test sets for LSTM experiments\n",
    "# ML_trn = ['p2_2022-08-03', 'p3_2022-08-04', 'p4_2022-08-10', \n",
    "#           'p6_2022-08-15', 'p7_2022-08-15', 'p8_2022-08-19', 'p10_2022-08-26', \n",
    "#           'p11_2022-08-29', 'p12_2022-08-29', 'p14_2022-08-31', 'p15_2022-09-02', \n",
    "#           'p16_2022-09-05', 'p18_2022-09-07', 'p19_2022-09-08', 'p20_2022-09-09']\n",
    "# ML_dev = ['p1_2022-07-25', 'p5_2022-08-12', 'p9_2022-08-22', 'p13_2022-08-29', 'p17_2022-09-06']\n",
    "# # for 5-fold cross-validation experiments\n",
    "# cv_1 = participants[0:4]\n",
    "# cv_2 = participants[4:8]\n",
    "# cv_3 = participants[8:12]\n",
    "# cv_4 = participants[12:16]\n",
    "# cv_5 = participants[16:]\n",
    "# ML_trn_cv_1 = cv_2 + cv_3 + cv_4 + cv_5\n",
    "# ML_tst_cv_1 = cv_1\n",
    "# ML_trn_cv_2 = cv_1 + cv_3 + cv_4 + cv_5\n",
    "# ML_tst_cv_2 = cv_2\n",
    "# ML_trn_cv_3 = cv_1 + cv_2 + cv_4 + cv_5\n",
    "# ML_tst_cv_3 = cv_3\n",
    "# ML_trn_cv_4 = cv_1 + cv_2 + cv_3 + cv_5\n",
    "# ML_tst_cv_4 = cv_4\n",
    "# ML_trn_cv_5 = cv_1 + cv_2 + cv_3 + cv_4\n",
    "# ML_tst_cv_5 = cv_5\n",
    "# list_comb_base = []\n",
    "# list_comb_arm = []\n",
    "# list_comb_otp = []\n",
    "\n",
    "# cv = 'ML_trn'\n",
    "# for participant in ML_trn:\n",
    "#     proc_data_each_base = 'data/ML/' + participant + '_processed_base.csv'\n",
    "#     proc_data_each_arm = 'data/ML/' + participant + '_processed_arm.csv'\n",
    "#     proc_data_each_otp = 'data/ML/' + participant + '_processed_otp.csv'\n",
    "#     df_each_base = pd.read_csv(proc_data_each_base, header = 0)\n",
    "#     df_each_arm = pd.read_csv(proc_data_each_arm, header = 0)\n",
    "#     df_each_otp = pd.read_csv(proc_data_each_otp, header = 0)\n",
    "#     list_comb_base.append(df_each_base)\n",
    "#     list_comb_arm.append(df_each_arm)\n",
    "#     list_comb_otp.append(df_each_otp)\n",
    "# df_comb_base = pd.concat(list_comb_base)\n",
    "# df_comb_arm = pd.concat(list_comb_arm)\n",
    "# df_comb_otp = pd.concat(list_comb_otp)\n",
    "# df_comb_base.to_csv(('data/ML/combined_base_'+cv+'.csv'), index=False)\n",
    "# df_comb_arm.to_csv(('data/ML/combined_arm_'+cv+'.csv'), index=False)\n",
    "# df_comb_otp.to_csv(('data/ML/combined_otp_'+cv+'.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
